# 数据库

## MySQL

### 基础语句

#### SELECT 语句语法

以下是 `SELECT` 语句的语法：

```sql
SELECT columns_list
FROM table_name;
```

说明：

- 关键字 `SELECT` 后跟着**一个或多个数据表的列**。
- `columns_list` 可以有多个列，他们之间需要用逗号 `,` 分隔。
- 当要检索数据表中的所有列的时候，使用 `SELECT * FROM table_name` 。
- 关键字 `FROM` 后跟着要从中检索数据的表名。
- 分号 `;` 表示语句的结束，它是可选的。如果有两条或更多条语句，则需要使用分号 `;` 将它们分开，以便 MySQL 单独执行每条语句。

## Pandas

### Pandas的数据结构

### DataFrame和Series

![image-20230823101856797](C:\Users\DYY\AppData\Roaming\Typora\typora-user-images\image-20230823101856797.png)

Series 是带标签的一维数组，可存储整数、浮点数、字符串、Python 对象等类型的数据。轴标签统称为**索引**。调用 `pd.Series` 函数即可创建 Series:

```python
s = pd.Series(data, index=index)
```

Series 可以用字典实例化：

```python
In [7]: d = {'b': 1, 'a': 0, 'c': 2}

In [8]: pd.Series(d)
Out[8]: 
b    1
a    0
c    2
dtype: int64
```

**DataFrame** 是由多种类型的列构成的二维标签数据结构，类似于 Excel 、SQL 表，或 Series 对象构成的字典。DataFrame 是最常用的 Pandas 对象，与 Series 一样，DataFrame 支持多种类型的输入数据：

- 一维 ndarray、列表、字典、Series 字典
- 二维 numpy.ndarray
- [结构多维数组或记录多维数组open in new window](https://docs.scipy.org/doc/numpy/user/basics.rec.html)
- `Series`
- `DataFrame`

除了数据，还可以有选择地传递 **index**（行标签）和 **columns**（列标签）参数。

### Pandas查询数据

1.df.loc方法，根据行、列的标签值查询
2.df.iloc方法，根据行列的数字位置查询
3.df.where方法
4.df.query方法

loc既能查询，又能覆盖写入，强烈推荐！

#### df.loc查询数据的方法

##### 1.使用单个`label`值查询数据

对于行和列都可以传入单个值实现精确匹配：

```python
# 得到单个值
df.loc['2015-05-12','气温(度)']

# 得到一个Series
df.loc['2015-05-12',['气温(度)','相对湿度(%)']]
```

##### 2.使用值列表批量查询

```python
# 得到Series
df.loc[['2015-05-12','2015-10-10'],'气温(度)']

# 得到DataFrame
df.loc[['2015-05-12','2015-10-10'],['气温(度)','相对湿度(%)']]
```

##### 3.使用数值区间进行范围查询

注意：区间既包含开始，也包含结束

```python
# 行index按区间
df.loc['2015-05-12':'2015-05-13','气温(度)']

# 列index按区间
df.loc['2015-10-10','气温(度)':'累积雨量(mm)']

# 行和列都按区间查询
df.loc['2015-05-12':'2015-05-13','气温(度)':'累积雨量(mm)']
```

##### 4.使用条件表达式查询

bool列表的长度得等于行数或者列数

**简单条件查询，最低温度低于-10度的列表**

```python
df.loc[df["气温(度)"]<-10,:]
```

**复杂查询条件，查一下我心中的目标天气**

```python
# 查询最高温度小于30度，最低温度大于15度的天气数据
df.loc[(df["气温(度)"].astype(float)<30) & (df["气温(度)"].astype(float)>15),:]
```

##### 5.调用函数查询

可以直接传入函数名

```
# 直接写lambda表达式
df.loc[lambda df : (df["气温(度)"].astype(float)<30) & (df["气温(度)"].astype(float)>15),:]
```

```python
#函数式编程的本质：
#    函数自身可以像变量一样传递

# 编写自己的函数，查询九月份累计雨量为0的数据
def query_my_data(df):
    return (df.index.str.startswith("2015-9")) & (df["累积雨量(mm)"]== 0.0)

df.loc[query_my_data,:]
```

### Pandas新增数据列

在进行数据分析时，经常需要按照一定条件创建新的数据列，然后进行下一步分析。

1. 直接赋值
2. df.apply方法
3. df.assign方法
4. 按条件选择分组分别赋值

#### 直接赋值

实例：清理温度列，变成数字类型

```python
# 替换掉温度的后缀℃
df.loc[:,"气温(度)"] = df["气温(度)"].str.replace("℃","").astype("float")
```

实例：计算温度差

```python
# 注意，df["气温(度)"]其实是一个Series，后面的减法返回的是Series
df.loc[:,"wencha"]=df["气温(度)"]-2
```

####  df.apply方法

Apply a function along an axis of the DataFrame.

Object passed to the function are Series objects whose index is either the DataFrame's index(axis=0) or the DataFrame's columns(axis=1).

实例：添加一列温度类型：

- 如果最高温度大于33度就是高温
- 低于-5度就是低温
- 否则就是常温

```python
def get_wendu_type(x):
    if x["气温(度)"] > 33:
        return '高温'
    if x["气温(度)"] < -10:
        return '低温'
    return '常温'

# 注意需要设置axis==1,这是Series的index是columns
df.loc[:,"wendu_type"] = df.apply(get_wendu_type,axis=1)
# 查看 温度类型的计数
df["wendu_type"].value_counts()
常温    473
低温     11
Name: wendu_type, dtype: int64
```

#### df.assign方法

Assign new columns to a DataFrame. Return a new object with all original columns in addition to new ones.

实例：将温度从摄氏度变成华氏度

```
# 可以同事添加多个新的列
df.assign(
    #摄氏度转华氏度
    tem_huashi = lambda x : x["气温(度)"] * 9 / 5 + 32
)
```

#### 按条件选择分组分别赋值

按条件先选择温度，然后对这部分数据赋值新列
实例：温度减去10度小于15度，则认为温差大

```python
# 先创建空列（这是第一种创建新列的方法）
df['wencha_type']=''
df.loc[df["气温(度)"]-10 < 15,"wencha_type"]="温差大"
df.loc[df["气温(度)"]-10 >= 15,"wencha_type"]="温差正常"
df["wencha_type"].value_counts()
```

### Pandas数据统计函数

1.汇总类统计
2.唯一去重和按值计数
3.相关系数和协方差

#### 汇总类统计

```python
# 提取所有数字列统计结果
df.describe()
```

```python
# 查看单个Series的数据
df["气温(度)"].mean()
16.73636363636366
# 最高温
df["气温(度)"].max()
30.6
# 最低温
df["气温(度)"].min()
```

#### 唯一去重和按值计数

##### 唯一性去重

一般不用于数值列，而是枚举、分类列

```python
df["城市"].unique
```

##### 按值计数

```python
df["气温(度)"].value_counts()
```

#### 相关系数和协方差

用途：

- 两只股票，是不是同涨同跌？程度多大？正相关还是负相关？
- 产品销量的波动，跟哪些因素正相关、负相关，程度有多大？

来自知乎，对于两个变量X,Y:

1. 协方差：**衡量同反向程度**，如果协方差为正，说明X,Y同向运动，协方差越大说明同向程度越高；如果协方差为负，说明X,Y反向运动，协方差越小说明反向程度越高。
2. 相关系数：**衡量相似度程度**，当他们的相关系数为1时，说明两个变量变化时的正向相似度最大，当相关系数为-1时，说明两个两个变量的反向相似度最大。

```python
# 协方差矩阵：
df.cov()
```

```python
# 相关系数矩阵：
df.corr()
```

|              | 气温(度) | 相对湿度(%) | 累积雨量(mm) |
| :----------- | :------- | :---------- | :----------- |
| 气温(度)     | 1.000000 | 0.884771    | 0.217571     |
| 相对湿度(%)  | 0.884771 | 1.000000    | 0.229271     |
| 累积雨量(mm) | 0.217571 | 0.229271    | 1.000000     |

```python
# 单独查看湿度和温度的相关系数
df["相对湿度(%)"].corr(df["气温(度)"])
0.8847707858193214
# ！！这就是特征工程对于机器学习重要性的一个例子
```

### Pandas缺失值处理

Pandas使用这些函数处理缺失值：

- isnull和notnull: 检测是否为空值，可用于df和Series
- dropna: 丢弃、删除缺失值
  - axis: 删除行还是列，{0 or 'index',1 or 'columns'},default 0;
  - how: 如果等于any则任何值为空都删除，如果等于all则所有值都为空才删除;
  - inplace: 如果为Ture则膝盖当前df，否则返回新的df;
- fillna：填充空值
  - value: 用于填充的值，可以是单个值，或者字典(key是列名，value是值);
  - method: 等于ffill使用前一个不为空的值填充forword fill;等于bfill使用后一个不为空的值填充backword fill;
  - axis: 按行还是列填充，{0 or 'index',1 or 'columns'};
  - inplace: 如果为True则修改当前df，否则返回新的df;

#### 实例：特殊Excel的读取、清洗、处理

##### 步骤一：读取excel的时候，忽略前几个空行

```python
studf = pd.read_excel("datas/student.xlsx",skiprows=2)
studf
```

|      | Unnamed: 0 | 姓名 | 科目 | 分数 |
| :--- | :--------- | :--- | :--- | :--- |
| 0    | NaN        | 小明 | 语文 | 85.0 |
| 1    | NaN        | NaN  | 数学 | 80.0 |
| 2    | NaN        | NaN  | 英语 | 90.0 |
| 3    | NaN        | NaN  | NaN  | NaN  |
| 4    | NaN        | 小王 | 语文 | 85.0 |
| 5    | NaN        | NaN  | 数学 | NaN  |
| 6    | NaN        | NaN  | 英语 | 90.0 |
| 7    | NaN        | NaN  | NaN  | NaN  |
| 8    | NaN        | 小刚 | 语文 | 85.0 |
| 9    | NaN        | NaN  | 数学 | 80.0 |
| 10   | NaN        | NaN  | 英语 | 90.0 |

##### 步骤2：检测空值

```python
studf.isnull()
```

|      | Unnamed: 0 | 姓名  | 科目  | 分数  |
| :--- | :--------- | :---- | :---- | :---- |
| 0    | True       | False | False | False |
| 1    | True       | True  | False | False |
| 2    | True       | True  | False | False |
| 3    | True       | True  | True  | True  |
| 4    | True       | False | False | False |
| 5    | True       | True  | False | True  |
| 6    | True       | True  | False | False |
| 7    | True       | True  | True  | True  |
| 8    | True       | False | False | False |
| 9    | True       | True  | False | False |
| 10   | True       | True  | False | False |

```python
studf["分数"].isnull()
0     False
1     False
2     False
3      True
4     False
5      True
6     False
7      True
8     False
9     False
10    False
Name: 分数, dtype: bool
studf["分数"].notnull()
0      True
1      True
2      True
3     False
4      True
5     False
6      True
7     False
8      True
9      True
10     True
Name: 分数, dtype: bool
# 筛选没有空分数的所有行
studf.loc[studf["分数"].notnull(),:]
```

|      | Unnamed: 0 | 姓名 | 科目 | 分数 |
| :--- | :--------- | :--- | :--- | :--- |
| 0    | NaN        | 小明 | 语文 | 85.0 |
| 1    | NaN        | NaN  | 数学 | 80.0 |
| 2    | NaN        | NaN  | 英语 | 90.0 |
| 4    | NaN        | 小王 | 语文 | 85.0 |
| 6    | NaN        | NaN  | 英语 | 90.0 |
| 8    | NaN        | 小刚 | 语文 | 85.0 |
| 9    | NaN        | NaN  | 数学 | 80.0 |
| 10   | NaN        | NaN  | 英语 | 90.0 |

##### 步骤3：删除全是空值的列

```python
studf.dropna(axis="columns",how='all',inplace=True)
studf
```

|      | 姓名 | 科目 | 分数 |
| :--- | :--- | :--- | :--- |
| 0    | 小明 | 语文 | 85.0 |
| 1    | NaN  | 数学 | 80.0 |
| 2    | NaN  | 英语 | 90.0 |
| 3    | NaN  | NaN  | NaN  |
| 4    | 小王 | 语文 | 85.0 |
| 5    | NaN  | 数学 | NaN  |
| 6    | NaN  | 英语 | 90.0 |
| 7    | NaN  | NaN  | NaN  |
| 8    | 小刚 | 语文 | 85.0 |
| 9    | NaN  | 数学 | 80.0 |
| 10   | NaN  | 英语 | 90.0 |

##### 步骤4：删除掉全是空值的行

```python
studf.dropna(axis="index",how='all',inplace=True)
studf
```

|      | 姓名 | 科目 | 分数 |
| :--- | :--- | :--- | :--- |
| 0    | 小明 | 语文 | 85.0 |
| 1    | NaN  | 数学 | 80.0 |
| 2    | NaN  | 英语 | 90.0 |
| 4    | 小王 | 语文 | 85.0 |
| 5    | NaN  | 数学 | NaN  |
| 6    | NaN  | 英语 | 90.0 |
| 8    | 小刚 | 语文 | 85.0 |
| 9    | NaN  | 数学 | 80.0 |
| 10   | NaN  | 英语 | 90.0 |

##### 步骤5：将分数列为空的填充为0分

```python
studf.fillna({"分数":0})
```

|      | 姓名 | 科目 | 分数 |
| :--- | :--- | :--- | :--- |
| 0    | 小明 | 语文 | 85.0 |
| 1    | NaN  | 数学 | 80.0 |
| 2    | NaN  | 英语 | 90.0 |
| 4    | 小王 | 语文 | 85.0 |
| 5    | NaN  | 数学 | 0.0  |
| 6    | NaN  | 英语 | 90.0 |
| 8    | 小刚 | 语文 | 85.0 |
| 9    | NaN  | 数学 | 80.0 |
| 10   | NaN  | 英语 | 90.0 |

```python
# 等同于
studf.loc[:,'分数'] = studf['分数'].fillna(0)
studf
```

|      | 姓名 | 科目 | 分数 |
| :--- | :--- | :--- | :--- |
| 0    | 小明 | 语文 | 85.0 |
| 1    | NaN  | 数学 | 80.0 |
| 2    | NaN  | 英语 | 90.0 |
| 4    | 小王 | 语文 | 85.0 |
| 5    | NaN  | 数学 | 0.0  |
| 6    | NaN  | 英语 | 90.0 |
| 8    | 小刚 | 语文 | 85.0 |
| 9    | NaN  | 数学 | 80.0 |
| 10   | NaN  | 英语 | 90.0 |

##### 步骤6：将姓名的缺失值填充

**使用前面的有效值自动填充null**，用ffill: forword fill

```python
studf.loc[:,"姓名"]=studf["姓名"].fillna(method="ffill")
studf
```

|      | 姓名 | 科目 | 分数 |
| :--- | :--- | :--- | :--- |
| 0    | 小明 | 语文 | 85.0 |
| 1    | 小明 | 数学 | 80.0 |
| 2    | 小明 | 英语 | 90.0 |
| 4    | 小王 | 语文 | 85.0 |
| 5    | 小王 | 数学 | 0.0  |
| 6    | 小王 | 英语 | 90.0 |
| 8    | 小刚 | 语文 | 85.0 |
| 9    | 小刚 | 数学 | 80.0 |
| 10   | 小刚 | 英语 | 90.0 |

##### 步骤7：将清洗好的excel保存

```python
studf.to_excel('datas/student_new.xlsx',index=False)#不保留自动生成的index
```

### Pandas数据排序

Series的排序：
***Series.sort_value(ascending=Ture,inplace=Flase)*** 参数说明：

- ascending: 默认为True升序排序，为False降序排序
- inplace: 是否修改原始Series

DataFrame的排序:
***DataFrame.sort_values(by,ascending=True,inplace=False)*** 参数说明：

- by: 字符串或者List<字符串>，单列排序或者多列排序
- ascending: bool或者list,升序还是降序，如果是list对应by的多列
- inplace: 是否修改原始DataFrame

#### 1. Series的排序

```python
# 升序排序
df["相对湿度(%)"].sort_values().head()
381   -9900
162   -9900
453   -9900
470   -9900
24    -9900
Name: 相对湿度(%), dtype: int64
# 降序排序
df["相对湿度(%)"].sort_values(ascending=False).head()
352    100
49     100
95     100
274    100
167    100
Name: 相对湿度(%), dtype: int64
df["行政区"].sort_values().head()
286     七堵區
329     七股區
411    三地門鄉
20      三峽區
276     三星鄉
Name: 行政区, dtype: object
```

#### 2. DataFrame的排序

##### 2.1 单列排序

```python
# 升序排序
df.sort_values(by="相对湿度(%)").head()
```

|      | 日期       | 城市   | 行政区 | 观测站 | 气温(度) | 相对湿度(%) | 累积雨量(mm) |
| :--- | :--------- | :----- | :----- | :----- | :------- | :---------- | :----------- |
| 381  | 2016-01-17 | 高雄市 | 梓官區 | 梓官   | -99.0    | -9900       | -99.0        |
| 162  | 2015-06-12 | 臺中市 | 烏日區 | 烏日   | -99.0    | -9900       | -99.0        |
| 453  | 2016-03-29 | 屏東縣 | 長治鄉 | 長治   | -99.0    | -9900       | 0.0          |
| 470  | 2016-04-15 | 新北市 | 永和區 | 永和   | -99.0    | -9900       | -99.0        |
| 24   | 2015-01-25 | 臺中市 | 西屯區 | 西屯   | -99.0    | -9900       | -99.0        |

```python
# 降序排序
df.sort_values(by="相对湿度(%)",ascending=False).head()
```

|      | 日期       | 城市   | 行政区 | 观测站 | 气温(度) | 相对湿度(%) | 累积雨量(mm) |
| :--- | :--------- | :----- | :----- | :----- | :------- | :---------- | :----------- |
| 49   | 2015-02-19 | 宜蘭縣 | 南澳鄉 | 西德山 | 9.1      | 100         | 4.5          |
| 392  | 2016-01-28 | 宜蘭縣 | 大同鄉 | 南山   | 10.4     | 100         | 0.0          |
| 21   | 2015-01-22 | 宜蘭縣 | 頭城鎮 | 桃源谷 | 11.3     | 100         | 20.0         |
| 384  | 2016-01-20 | 花蓮縣 | 鳳林鎮 | 鳳林山 | 14.3     | 100         | 2.5          |
| 70   | 2015-03-12 | 臺北市 | 北投區 | 大屯山 | 7.3      | 100         | 6.0          |

##### 2.2 多列排序

```python
# 按湿度、温度排序，默认升序
df.sort_values(by=["相对湿度(%)","气温(度)"]).head()
```

|      | 日期       | 城市   | 行政区 | 观测站    | 气温(度) | 相对湿度(%) | 累积雨量(mm) |
| :--- | :--------- | :----- | :----- | :-------- | :------- | :---------- | :----------- |
| 15   | 2015-01-16 | 苗栗縣 | 造橋鄉 | 造橋      | -99.0    | -9900       | -99.0        |
| 24   | 2015-01-25 | 臺中市 | 西屯區 | 西屯      | -99.0    | -9900       | -99.0        |
| 52   | 2015-02-22 | 南投縣 | 仁愛鄉 | 奇萊稜線A | -99.0    | -9900       | 0.0          |
| 132  | 2015-05-13 | 臺南市 | 西港區 | 西港      | -99.0    | -9900       | 0.0          |
| 158  | 2015-06-08 | 屏東縣 | 新埤鄉 | 新埤      | -99.0    | -9900       | -99.0        |

```python
# 两个字段都是降序
df.sort_values(by=["相对湿度(%)","气温(度)"],ascending=False).head()
```

|      | 日期       | 城市   | 行政区 | 观测站 | 气温(度) | 相对湿度(%) | 累积雨量(mm) |
| :--- | :--------- | :----- | :----- | :----- | :------- | :---------- | :----------- |
| 167  | 2015-06-17 | 彰化縣 | 芳苑鄉 | 芳苑   | 19.5     | 100         | 0.0          |
| 399  | 2016-02-04 | 臺東縣 | 大武鄉 | 山豬窟 | 19.0     | 100         | 0.0          |
| 291  | 2015-10-19 | 花蓮縣 | 玉里鎮 | 德武   | 17.8     | 100         | 1.5          |
| 292  | 2015-10-20 | 花蓮縣 | 瑞穗鄉 | 舞鶴   | 16.8     | 100         | 1.0          |
| 111  | 2015-04-22 | 花蓮縣 | 光復鄉 | 富源   | 16.5     | 100         | 1.0          |

```python
#分别指定升序和降序
df.sort_values(by=["相对湿度(%)","气温(度)"],ascending=[True,False]).head()
```

|      | 日期       | 城市   | 行政区 | 观测站    | 气温(度) | 相对湿度(%) | 累积雨量(mm) |
| :--- | :--------- | :----- | :----- | :-------- | :------- | :---------- | :----------- |
| 104  | 2015-04-15 | 臺中市 | 北屯區 | 大坑      | 22.8     | -9900       | 0.0          |
| 286  | 2015-10-14 | 基隆市 | 七堵區 | 七堵      | 12.6     | -9900       | 27.0         |
| 15   | 2015-01-16 | 苗栗縣 | 造橋鄉 | 造橋      | -99.0    | -9900       | -99.0        |
| 24   | 2015-01-25 | 臺中市 | 西屯區 | 西屯      | -99.0    | -9900       | -99.0        |
| 52   | 2015-02-22 | 南投縣 | 仁愛鄉 | 奇萊稜線A | -99.0    | -9900       | 0.0          |

### Pandas字符串操作

前面我们已经使用了字符串的处理函数：
df["气温(度)"].str.replace("℃","").astype("float")

***Pandas的字符串处理:***

1. 使用方法: 先获取Series的str属性，然后再属性上调用函数;
2. 只能再字符串列上使用，不能再数字列上使用;
3. DataFrame上没有str属性和处理方法;
4. Series.str并不是Python原生字符串，而是自己的一套方法，不过大部分和原生str很相似;

***Series.str字符串方法列表参考文档:***
[https://pandas.pydata.org/docs/reference/series.html](https://gitee.com/link?target=https%3A%2F%2Fpandas.pydata.org%2Fdocs%2Freference%2Fseries.html)

***本节演示内容:***

1. 获取Series的str属性，然后使用各种字符串处理函数
2. 使用str的startswith、contains等boolean类Series可以做条件查询
3. 需要多次str处理的链式操作
4. 使用正则表达式处理

#### 1. 获取Series的str属性，使用各种字符串处理函数

```python
df["气温(度)"].str
```

```python
df["气温(度)"].str.replace("℃","").head()
0    13.7
1    23.5
2    19.6
3    14.2
4     8.3
Name: 气温(度), dtype: object
# 判断是不是数字
df["气温(度)"].str.isnumeric().head()
```

#### 2. 使用str的startswith、contains等得到bool的Series可以做条件查询

```
condition=df["日期"].str.startswith("2015-11")
```

#### 3. 需要多次str处理的链式操作

怎么提取201811这样的数字月份？
1.现将日期2018-11-20替换成20181120的形式
2.提取月份字符串201811

```python
df["日期"].str.replace("-","").head()
```

```python
# 每次调用函数，都返回一个新的Series
df["日期"].str.replace("-","").str.slice(0,6).head()
```

```python
# slice就是切片语法，可以直接用
df["日期"].str.replace("-","").str[0:6].head()
```

#### 4. 使用正则表达式处理

```python
# 添加新列
def get_nianyueri(x):
    year,month,day = x["日期"].split("-")#str.split(str=“”，num=)将字符串按照分隔符分开
    return f"{year}年{month}月{day}日"
df["中文日期"] = df.apply(get_nianyueri,axis=1)
df["中文日期"].head()
```

问题：怎么将"2015年11月20日"中的年、月、日三个中文字符去除？

```python
# 方法1：链式replace
df["中文日期"].str.replace("年","").str.replace("月","").str.replace("日","").head()
```

***Series.str默认就开启了正则表达式模式***

```python
# 方法二：正则表达式替换
df["中文日期"].str.replace("年月日","").head()
```

### Pandas的axis参数

- axis=0 或者"index"
- 如果是单行操作，就指的是某一行
- 如果是聚合操作，指的是跨行cross rows
- axis=1或者"columns":
- 如果是单列操作，就指的是某一列
- 如果是聚合操作，指的是跨列cross columns

***按哪个axis，就是这个axis要动起来(类似被for遍历)，其他axis保持不动***

```python
df = pd.DataFrame(
    np.arange(12).reshape(3,4),
    columns=['A','B','C','D']
)
df
```

#### 1. 单列drop，就是删除某一列

```python
# 代表的就是删除某列
df.drop("A",axis=1)
```

|      | B    | C    | D    |
| :--- | :--- | :--- | :--- |
| 0    | 1    | 2    | 3    |
| 1    | 5    | 6    | 7    |
| 2    | 9    | 10   | 11   |

#### 2.单行drop,就是删除某一行

```python
df.drop(1,axis=0)
```

|      | A    | B    | C    | D    |
| :--- | :--- | :--- | :--- | :--- |
| 0    | 0    | 1    | 2    | 3    |
| 2    | 8    | 9    | 10   | 11   |

#### 3. 按axis=0/index执行mean聚合操作

反直觉：输出的不是每行的结果，而是每列的结果

```
df
```

|      | A    | B    | C    | D    |
| :--- | :--- | :--- | :--- | :--- |
| 0    | 0    | 1    | 2    | 3    |
| 1    | 4    | 5    | 6    | 7    |
| 2    | 8    | 9    | 10   | 11   |

```python
# axis=0 or axis=index
df.mean(axis=0)
A    4.0
B    5.0
C    6.0
D    7.0
dtype: float64
```

***指定了按哪个axis，就是这个axis要动起来(类似被for遍历)，其他axis保持不动***

#### 4. 按axis=1/columns执行mean聚合操作

反直觉：输出的不是每行的结果，而是每列的结果

```
df
```

|      | A    | B    | C    | D    |
| :--- | :--- | :--- | :--- | :--- |
| 0    | 0    | 1    | 2    | 3    |
| 1    | 4    | 5    | 6    | 7    |
| 2    | 8    | 9    | 10   | 11   |

```python
# axis=1 or axis=columns
df.mean(axis=1)
0    1.5
1    5.5
2    9.5
dtype: float64
```

***指定了按哪个axis，就是这个axis要动起来(类似被for遍历)，其他axis保持不动***

```python
## 5. 再次举例，加深理解
def get_sum_value(x):
    return x["A"] + x["B"] + x["C"] + x["D"]

df["sum_value"] = df.apply(get_sum_value,axis=1)
df
```

|      | A    | B    | C    | D    | sum_value |
| :--- | :--- | :--- | :--- | :--- | :-------- |
| 0    | 0    | 1    | 2    | 3    | 6         |
| 1    | 4    | 5    | 6    | 7    | 22        |
| 2    | 8    | 9    | 10   | 11   | 38        |

***指定了按哪个axis，就是这个axis要动起来(类似被for遍历)，其他axis保持不动***

```python
# axis=0 or axis=index
df.mean(axis=0)xxxxxxxxxx df# axis=0 or axis=indexdf.mean(axis=0)
```

### Pandas的Merge

Pandas的Merge，相当于SQL的join操作，将不同的表按key关联到一个表

**merge语法：**

merge( left,
right,
how="inner",
on=None,
left_on=None,
right_on=None,
left_index=False,
right_index=False,
sort=False,
suffixes=("_x", "_y"),
copy=True,
indicator=False,
validate=None,
)

- left,right: 要merge的DataFrame或者有name的Series;
- how: join类型,'left','right','outer','inner';
- on: join的key,left和right都需要有这个key;
- left_on: left的df或者series的key;
- right_on: right的df或者series的key;
- left_index,right_index: 使用index而不是普通的column做join;
- sort: 默认为False，将合并的数据进行排序;
- sufflxes: 两个元素的后缀,如果有重名,自动添加后缀,默认是('_x','_y');
- copy: 默认为True，总是将数据复制到数据结构中，设置为False可以提高性能;
- indicator: 显示合并数据中数据来自哪个表;

#### 1. 电影数据集的join实例

**电影评分数据集** 是推荐系统研究的很好的数据集

包含三个文件:

1. 用户对电影的评分数据: ratings.dat
2. 用户本身的信息数据: users.dat
3. 电影本身的数据: movies.dat

可以关联三个表，得到一个完整的大表

数据集官方地址: [https://grouplens.org/datasets/movielens/](https://gitee.com/link?target=https%3A%2F%2Fgrouplens.org%2Fdatasets%2Fmovielens%2F)

```
import pandas as pd
df_ratings = pd.read_csv(
"../datas/mov/ratings.dat",
    sep="::",
    engine="python",
    names="UserID::MovieID::Rating::Timestamp".split("::")
)
df_ratings.head()
```

|      | UserID | MovieID | Rating | Timestamp |
| :--- | :----- | :------ | :----- | :-------- |
| 0    | 1      | 1193    | 5      | 978300760 |
| 1    | 1      | 661     | 3      | 978302109 |
| 2    | 1      | 914     | 3      | 978301968 |
| 3    | 1      | 3408    | 4      | 978300275 |
| 4    | 1      | 2355    | 5      | 978824291 |

```
df_users = pd.read_csv(
"../datas/mov/users.dat",
    sep="::",
    engine="python",
    names="UserID::Gender::Age::Occupation::Zip-code".split("::")
)
df_users.head()
```

|      | UserID | Gender | Age  | Occupation | Zip-code |
| :--- | :----- | :----- | :--- | :--------- | :------- |
| 0    | 1      | F      | 1    | 10         | 48067    |
| 1    | 2      | M      | 56   | 16         | 70072    |
| 2    | 3      | M      | 25   | 15         | 55117    |
| 3    | 4      | M      | 45   | 7          | 02460    |
| 4    | 5      | M      | 25   | 20         | 55455    |

```
df_movies = pd.read_csv(
"../datas/mov/movies.dat",
    sep="::",
    engine="python",
    names="MovieID::Title::Genres".split("::")
)
df_movies.head()
```

|      | MovieID | Title                              | Genres                         |
| :--- | :------ | :--------------------------------- | :----------------------------- |
| 0    | 1       | Toy Story (1995)                   | Animation\|Children's\|Comedy  |
| 1    | 2       | Jumanji (1995)                     | Adventure\|Children's\|Fantasy |
| 2    | 3       | Grumpier Old Men (1995)            | Comedy\|Romance                |
| 3    | 4       | Waiting to Exhale (1995)           | Comedy\|Drama                  |
| 4    | 5       | Father of the Bride Part II (1995) | Comedy                         |

```
df_ratings_users = pd.merge(
    df_ratings,df_users,left_on="UserID",right_on="UserID",how="inner"
)
df_ratings_users.head()
```

|      | UserID | MovieID | Rating | Timestamp | Gender | Age  | Occupation | Zip-code |
| :--- | :----- | :------ | :----- | :-------- | :----- | :--- | :--------- | :------- |
| 0    | 1      | 1193    | 5      | 978300760 | F      | 1    | 10         | 48067    |
| 1    | 1      | 661     | 3      | 978302109 | F      | 1    | 10         | 48067    |
| 2    | 1      | 914     | 3      | 978301968 | F      | 1    | 10         | 48067    |
| 3    | 1      | 3408    | 4      | 978300275 | F      | 1    | 10         | 48067    |
| 4    | 1      | 2355    | 5      | 978824291 | F      | 1    | 10         | 48067    |

```
df_ratings_users_movies = pd.merge(
    df_ratings_users,df_movies,left_on="MovieID",right_on="MovieID",how="inner"
)
df_ratings_users_movies.head()
```

|      | UserID | MovieID | Rating | Timestamp | Gender | Age  | Occupation | Zip-code | Title                                  | Genres |
| :--- | :----- | :------ | :----- | :-------- | :----- | :--- | :--------- | :------- | :------------------------------------- | :----- |
| 0    | 1      | 1193    | 5      | 978300760 | F      | 1    | 10         | 48067    | One Flew Over the Cuckoo's Nest (1975) | Drama  |
| 1    | 2      | 1193    | 5      | 978298413 | M      | 56   | 16         | 70072    | One Flew Over the Cuckoo's Nest (1975) | Drama  |
| 2    | 12     | 1193    | 4      | 978220179 | M      | 25   | 12         | 32793    | One Flew Over the Cuckoo's Nest (1975) | Drama  |
| 3    | 15     | 1193    | 4      | 978199279 | M      | 25   | 7          | 22903    | One Flew Over the Cuckoo's Nest (1975) | Drama  |
| 4    | 17     | 1193    | 5      | 978158471 | M      | 50   | 1          | 95350    | One Flew Over the Cuckoo's Nest (1975) | Drama  |

#### 2. 理解merge数量的对齐关系

以下关系要正确理解：

- one-to-one: 一对一关系，关联的key都是唯一的;
- 比如(学号，姓名) merge (学号，年龄);
- 结果条数: 1*1
- one-to-many: 一对多关系，左边唯一key，右边不唯一key;
- 比如(学号，姓名) merge (学号，[语文成绩，数学成绩，英语成绩]);
- 结果条数: 1*N
- many-to-many: 多对多关系，左边和右边都不唯一;
- 比如(学号，[语文成绩，数学成绩，英语成绩]) merge (学号，[篮球、足球、乒乓球])
- 结果条数: M*N

##### 2.1 one-to-one merge

```
left = pd.DataFrame({'sno':[11,12,13,14],
    'name':['n_a','n_b','n_c','n_d']
})
left
```

|      | sno  | name |
| :--- | :--- | :--- |
| 0    | 11   | n_a  |
| 1    | 12   | n_b  |
| 2    | 13   | n_c  |
| 3    | 14   | n_d  |

```
right = pd.DataFrame({'sno':[11,12,13,14],
    'age':['21','22','23','24']
})
right
```

|      | sno  | age  |
| :--- | :--- | :--- |
| 0    | 11   | 21   |
| 1    | 12   | 22   |
| 2    | 13   | 23   |
| 3    | 14   | 24   |

```
# 一对一关系，结果中有4条
pd.merge(left,right,on='sno')
```

|      | sno  | name | age  |
| :--- | :--- | :--- | :--- |
| 0    | 11   | n_a  | 21   |
| 1    | 12   | n_b  | 22   |
| 2    | 13   | n_c  | 23   |
| 3    | 14   | n_d  | 24   |

##### 2.2 one-to-many

**注意: 数据会被复制**

```
left = pd.DataFrame({'sno':[11,12,13,14],
    'name':['n_a','n_b','n_c','n_d']
})
left
```

|      | sno  | name |
| :--- | :--- | :--- |
| 0    | 11   | n_a  |
| 1    | 12   | n_b  |
| 2    | 13   | n_c  |
| 3    | 14   | n_d  |

```
right = pd.DataFrame({'sno':[11,11,11,12,12,13],
    'grade':['语文88','数学90','英语75','语文66','数学80','英语58']
})
right
```

|      | sno  | grade  |
| :--- | :--- | :----- |
| 0    | 11   | 语文88 |
| 1    | 11   | 数学90 |
| 2    | 11   | 英语75 |
| 3    | 12   | 语文66 |
| 4    | 12   | 数学80 |
| 5    | 13   | 英语58 |

```
# 数目以多的一边为准
pd.merge(left,right,on='sno')
```

|      | sno  | name | grade  |
| :--- | :--- | :--- | :----- |
| 0    | 11   | n_a  | 语文88 |
| 1    | 11   | n_a  | 数学90 |
| 2    | 11   | n_a  | 英语75 |
| 3    | 12   | n_b  | 语文66 |
| 4    | 12   | n_b  | 数学80 |
| 5    | 13   | n_c  | 英语58 |

##### 2.3 many-to-many

**注意：结果数量会等于左右两边数目的乘积**

```
left = pd.DataFrame({'sno':[11,11,12,12,12],
    '爱好':['篮球','羽毛球','乒乓球','篮球','足球']
})
left
```

|      | sno  | 爱好   |
| :--- | :--- | :----- |
| 0    | 11   | 篮球   |
| 1    | 11   | 羽毛球 |
| 2    | 12   | 乒乓球 |
| 3    | 12   | 篮球   |
| 4    | 12   | 足球   |

```
right = pd.DataFrame({'sno':[11,11,11,12,12,13],
    'grade':['语文88','数学90','英语75','语文66','数学80','英语58']
})
right
```

|      | sno  | grade  |
| :--- | :--- | :----- |
| 0    | 11   | 语文88 |
| 1    | 11   | 数学90 |
| 2    | 11   | 英语75 |
| 3    | 12   | 语文66 |
| 4    | 12   | 数学80 |
| 5    | 13   | 英语58 |

```
pd.merge(left,right,on='sno')
```

|      | sno  | 爱好   | grade  |
| :--- | :--- | :----- | :----- |
| 0    | 11   | 篮球   | 语文88 |
| 1    | 11   | 篮球   | 数学90 |
| 2    | 11   | 篮球   | 英语75 |
| 3    | 11   | 羽毛球 | 语文88 |
| 4    | 11   | 羽毛球 | 数学90 |
| 5    | 11   | 羽毛球 | 英语75 |
| 6    | 12   | 乒乓球 | 语文66 |
| 7    | 12   | 乒乓球 | 数学80 |
| 8    | 12   | 篮球   | 语文66 |
| 9    | 12   | 篮球   | 数学80 |
| 10   | 12   | 足球   | 语文66 |
| 11   | 12   | 足球   | 数学80 |

#### 3. 理解left join/right join/inner join/outer join的区别

![img](https://gitee.com/slient_7/study-pandas/raw/master/file/pics/join.png)

```
left = pd.DataFrame({'key':['K0','K1','K2','K3'],
                     'A':['A0','A1','A2','A3'],
                     'B':['B0','B1','B2','B3']})

right = pd.DataFrame({'key':['K0','K1','K4','K5'],
                     'C':['C0','C1','C4','C5'],
                     'D':['D0','D1','D4','D5']})
left
```

|      | key  | A    | B    |
| :--- | :--- | :--- | :--- |
| 0    | K0   | A0   | B0   |
| 1    | K1   | A1   | B1   |
| 2    | K2   | A2   | B2   |
| 3    | K3   | A3   | B3   |

```
right
```

|      | key  | C    | D    |
| :--- | :--- | :--- | :--- |
| 0    | K0   | C0   | D0   |
| 1    | K1   | C1   | D1   |
| 2    | K4   | C4   | D4   |
| 3    | K5   | C5   | D5   |

##### 3.1 inner join

左边和右边的key都有，才会出现在结果里

```
pd.merge(left,right,how='inner')
```

|      | key  | A    | B    | C    | D    |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | K0   | A0   | B0   | C0   | D0   |
| 1    | K1   | A1   | B1   | C1   | D1   |

##### 3.2 left join

左边的都会出现在结果里，右边的入如果无法匹配则为NULL

```
pd.merge(left,right,how='left')
```

|      | key  | A    | B    | C    | D    |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | K0   | A0   | B0   | C0   | D0   |
| 1    | K1   | A1   | B1   | C1   | D1   |
| 2    | K2   | A2   | B2   | NaN  | NaN  |
| 3    | K3   | A3   | B3   | NaN  | NaN  |

##### 3.3 right join

右边的都会出现在结果里，左边的入如果无法匹配则为NULL

```
pd.merge(left,right,how='right')
```

|      | key  | A    | B    | C    | D    |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | K0   | A0   | B0   | C0   | D0   |
| 1    | K1   | A1   | B1   | C1   | D1   |
| 2    | K4   | NaN  | NaN  | C4   | D4   |
| 3    | K5   | NaN  | NaN  | C5   | D5   |

##### 3.4 outer join

左边右边的都会出现在结果里，如果无法匹配则为NULL

```
pd.merge(left,right,how='outer')
```

|      | key  | A    | B    | C    | D    |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | K0   | A0   | B0   | C0   | D0   |
| 1    | K1   | A1   | B1   | C1   | D1   |
| 2    | K2   | A2   | B2   | NaN  | NaN  |
| 3    | K3   | A3   | B3   | NaN  | NaN  |
| 4    | K4   | NaN  | NaN  | C4   | D4   |
| 5    | K5   | NaN  | NaN  | C5   | D5   |

#### 4. 如果出现非Key的字段重名怎么办

```
left = pd.DataFrame({'key':['K0','K1','K2','K3'],
                     'A':['A0','A1','A2','A3'],
                     'B':['B0','B1','B2','B3']})

right = pd.DataFrame({'key':['K0','K1','K4','K5'],
                     'A':['A10','A11','A12','A13'],
                     'D':['D0','D1','D4','D5']})
left
```

|      | key  | A    | B    |
| :--- | :--- | :--- | :--- |
| 0    | K0   | A0   | B0   |
| 1    | K1   | A1   | B1   |
| 2    | K2   | A2   | B2   |
| 3    | K3   | A3   | B3   |

```
right
```

|      | key  | A    | D    |
| :--- | :--- | :--- | :--- |
| 0    | K0   | A10  | D0   |
| 1    | K1   | A11  | D1   |
| 2    | K4   | A12  | D4   |
| 3    | K5   | A13  | D5   |

```
pd.merge(left,right,on='key')
```

|      | key  | A_x  | B    | A_y  | D    |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | K0   | A0   | B0   | A10  | D0   |
| 1    | K1   | A1   | B1   | A11  | D1   |

```
# 指定后缀
pd.merge(left,right,on='key',suffixes=('_left','_right'))
```

|      | key  | A_left | B    | A_right | D    |
| :--- | :--- | :----- | :--- | :------ | :--- |
| 0    | K0   | A0     | B0   | A10     | D0   |
| 1    | K1   | A1     | B1   | A11     | D1   |

### Pandas数据合并concat

**使用场景:** 批量合并相同格式的Excel、给DataFrame添加行、给DataFrame添加列

**一句话说明concat语法:**

- 使用某种合并方式(inner/outer);
- 沿着某个轴向(axis=0/1);
- 把多个Pandas对象(DataFrame/Series)合并成一个;

**concat语法：pandas.concat(objs,axis=0,join='outer',ignore_index=False)**

- objs: 一个列表，内容可以是DataFrame或者Series，可以混合;
- axis: 默认是0代表按行合并，如果等于1代表按列合并;
- join: 合并的时候索引的对齐方式，默认是outer join，也可以是inner join;
- ignore_index: 是否忽略掉原来的数据索引;

**append语法：DataFrame.append(other,ignore_index=False)**

append只有按行合并，没有按列合并，**相当于concat按行的简写形式**

- other: 单个dataframe、series、dict，或者列表;
- ignore_index: 是否忽略掉原来的数据索引;

```python
import pandas as pd

import warnings
warnings.filterwarnings('ignore')
```

#### 一. 使用pandas.concat合并数据

```
df1 = pd.DataFrame({'A':['A0','A1','A2','A3'],
                    'B':['B0','B1','B2','B3'],
                    'C':['C0','C1','C2','C3'],
                    'D':['D0','D1','D2','D3'],
                    'E':['E0','E1','E2','E3']
                   })
                    
df1
```

|      | A    | B    | C    | D    | E    |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | A0   | B0   | C0   | D0   | E0   |
| 1    | A1   | B1   | C1   | D1   | E1   |
| 2    | A2   | B2   | C2   | D2   | E2   |
| 3    | A3   | B3   | C3   | D3   | E3   |

```
df2 = pd.DataFrame({'A':['A4','A5','A6','A7'],
                    'B':['B4','B5','B6','B7'],
                    'C':['C4','C5','C6','C7'],
                    'D':['D4','D5','D6','D7'],
                    'F':['F4','F5','F6','F7']
                   })
                    
df2
```

|      | A    | B    | C    | D    | F    |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | A4   | B4   | C4   | D4   | F4   |
| 1    | A5   | B5   | C5   | D5   | F5   |
| 2    | A6   | B6   | C6   | D6   | F6   |
| 3    | A7   | B7   | C7   | D7   | F7   |

***1. 默认的concat,参数为axis=0、join=outer、ignore_index=Flase***

```
pd.concat([df1,df2])
```

|      | A    | B    | C    | D    | E    | F    |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | A0   | B0   | C0   | D0   | E0   | NaN  |
| 1    | A1   | B1   | C1   | D1   | E1   | NaN  |
| 2    | A2   | B2   | C2   | D2   | E2   | NaN  |
| 3    | A3   | B3   | C3   | D3   | E3   | NaN  |
| 0    | A4   | B4   | C4   | D4   | NaN  | F4   |
| 1    | A5   | B5   | C5   | D5   | NaN  | F5   |
| 2    | A6   | B6   | C6   | D6   | NaN  | F6   |
| 3    | A7   | B7   | C7   | D7   | NaN  | F7   |

***2. 使用ignore_index=True可以忽略原来的索引***

```
pd.concat([df1,df2],ignore_index=True)
```

|      | A    | B    | C    | D    | E    | F    |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | A0   | B0   | C0   | D0   | E0   | NaN  |
| 1    | A1   | B1   | C1   | D1   | E1   | NaN  |
| 2    | A2   | B2   | C2   | D2   | E2   | NaN  |
| 3    | A3   | B3   | C3   | D3   | E3   | NaN  |
| 4    | A4   | B4   | C4   | D4   | NaN  | F4   |
| 5    | A5   | B5   | C5   | D5   | NaN  | F5   |
| 6    | A6   | B6   | C6   | D6   | NaN  | F6   |
| 7    | A7   | B7   | C7   | D7   | NaN  | F7   |

***3. 使用join=inner过滤掉不匹配的列***

```
pd.concat([df1,df2],ignore_index=True,join="inner")
```

|      | A    | B    | C    | D    |
| :--- | :--- | :--- | :--- | :--- |
| 0    | A0   | B0   | C0   | D0   |
| 1    | A1   | B1   | C1   | D1   |
| 2    | A2   | B2   | C2   | D2   |
| 3    | A3   | B3   | C3   | D3   |
| 4    | A4   | B4   | C4   | D4   |
| 5    | A5   | B5   | C5   | D5   |
| 6    | A6   | B6   | C6   | D6   |
| 7    | A7   | B7   | C7   | D7   |

***4. 使用axis=1相当于添加新列***

```
df1
```

|      | A    | B    | C    | D    | E    |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | A0   | B0   | C0   | D0   | E0   |
| 1    | A1   | B1   | C1   | D1   | E1   |
| 2    | A2   | B2   | C2   | D2   | E2   |
| 3    | A3   | B3   | C3   | D3   | E3   |

***A: 添加一列Series***

```
s1 = pd.Series(list(range(4)),name='F')
pd.concat([df1,s1],axis=1)
```

|      | A    | B    | C    | D    | E    | F    |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0    | A0   | B0   | C0   | D0   | E0   | 0    |
| 1    | A1   | B1   | C1   | D1   | E1   | 1    |
| 2    | A2   | B2   | C2   | D2   | E2   | 2    |
| 3    | A3   | B3   | C3   | D3   | E3   | 3    |

***B: 添加多列Series***

```
s2 = df1.apply(lambda x:x["A"]+"_GG",axis=1)
s2
0    A0_GG
1    A1_GG
2    A2_GG
3    A3_GG
dtype: object
s2.name="G"
pd.concat([df1,s1,s2],axis=1)
```

|      | A    | B    | C    | D    | E    | F    | G     |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---- |
| 0    | A0   | B0   | C0   | D0   | E0   | 0    | A0_GG |
| 1    | A1   | B1   | C1   | D1   | E1   | 1    | A1_GG |
| 2    | A2   | B2   | C2   | D2   | E2   | 2    | A2_GG |
| 3    | A3   | B3   | C3   | D3   | E3   | 3    | A3_GG |

```
# 列表可以只有Series
pd.concat([s1,s2],axis=1)
```

|      | F    | G     |
| :--- | :--- | :---- |
| 0    | 0    | A0_GG |
| 1    | 1    | A1_GG |
| 2    | 2    | A2_GG |
| 3    | 3    | A3_GG |

```
# 列表是可以混合排序的
pd.concat([s1,df1,s2],axis=1)
```

|      | F    | A    | B    | C    | D    | E    | G     |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---- |
| 0    | 0    | A0   | B0   | C0   | D0   | E0   | A0_GG |
| 1    | 1    | A1   | B1   | C1   | D1   | E1   | A1_GG |
| 2    | 2    | A2   | B2   | C2   | D2   | E2   | A2_GG |
| 3    | 3    | A3   | B3   | C3   | D3   | E3   | A3_GG |

#### 二. 使用DataFrame.append按行合并数据

```
df1 = pd.DataFrame([[1,2],[3,4]],columns=list('AB'))
df1
```

|      | A    | B    |
| :--- | :--- | :--- |
| 0    | 1    | 2    |
| 1    | 3    | 4    |

```
df2 = pd.DataFrame([[5,6],[7,8]],columns=list('AB'))
df2
```

|      | A    | B    |
| :--- | :--- | :--- |
| 0    | 5    | 6    |
| 1    | 7    | 8    |

***1. 给1个DataFrame添加给另一个DataFrame***

```
df1.append(df2)
```

|      | A    | B    |
| :--- | :--- | :--- |
| 0    | 1    | 2    |
| 1    | 3    | 4    |
| 0    | 5    | 6    |
| 1    | 7    | 8    |

***2. 忽略原来的索引ignore_index=True***

```
df1.append(df2,ignore_index=True)
```

|      | A    | B    |
| :--- | :--- | :--- |
| 0    | 1    | 2    |
| 1    | 3    | 4    |
| 2    | 5    | 6    |
| 3    | 7    | 8    |

***3. 可以一行一行的给DataFrame添加数据***

```
# 一个空的df
df = pd.DataFrame(columns=['A'])
df
```

|      | A    |
| :--- | :--- |
|      |      |

***A: 低性能版本***

```
for i in range(5):
    # 注意这里每次都在复制
    df = df.append({'A':i},ignore_index=True)
df
```

|      | A    |
| :--- | :--- |
| 0    | 0    |
| 1    | 1    |
| 2    | 2    |
| 3    | 3    |
| 4    | 4    |

***B: 高性能版本***

```
# 第一个入参是一个列表，避免了多次复制
pd.concat(
    [pd.DataFrame([i],columns=['A']) for i in range(5) ],
    ignore_index=True
)
```

|      | A    |
| :--- | :--- |
| 0    | 0    |
| 1    | 1    |
| 2    | 2    |
| 3    | 3    |
| 4    | 4    |

### Pandas实现groupby分组

类似SQL:
select student,sum(grade) from stu_grade group by student;

groupby: 先对数据分组，之后再每个分组上应用聚合函数、转换函数

------

演示内容：

1. 分组使用聚合函数做数据统计;
2. 遍历groupby的结果理解执行流程;
3. 实例分组探索天气数据;

```python
import pandas as pd
import numpy as np
# 加上这句，能在juypter notebook展示matplot图表
%matplotlib inline
df = pd.DataFrame({'A': ['foo','bar','foo','bar','foo','bar','foo','foo'],
                   'B': ['one','one','two','three','two','two','one','three'],
                   'C': np.random.randn(8),
                   'D': np.random.randn(8)})
df
```

|      |    A |     B |         C |         D |
| :--- | ---: | ----: | --------: | --------: |
| 0    |  foo |   one |  0.953909 | -1.749344 |
| 1    |  bar |   one | -0.065097 |  0.066270 |
| 2    |  foo |   two |  1.679496 | -1.617316 |
| 3    |  bar | three |  0.302761 |  0.505803 |
| 4    |  foo |   two | -1.015058 | -0.899095 |
| 5    |  bar |   two | -0.415399 |  0.549753 |
| 6    |  foo |   one | -0.831433 | -1.157519 |
| 7    |  foo | three | -0.285010 | -1.067717 |

#### 一. 分组使用聚合函数做数据统计

##### 1.1. 单个列groupby，查询所有数据列的统计

```
df.groupby('A').sum()
```

|      |         C |         D |
| :--- | --------: | --------: |
| A    |           |           |
| bar  | -0.177735 |  1.121826 |
| foo  |  0.501904 | -6.490991 |

可以看到：

1. groupby中的'A'变成了数据的索引列;
2. 因为要统计sum，但B列不是数字，所以被自动忽略掉;

##### 1.2. 多个列groupby，查询所有数据列的统计

```
df.groupby(['A','B']).mean()
```

|       |           |         C |         D |
| :---- | :-------- | --------: | --------: |
| A     | B         |           |           |
| bar   | one       | -0.065097 |  0.066270 |
| three | 0.302761  |  0.505803 |           |
| two   | -0.415399 |  0.549753 |           |
| foo   | one       |  0.061238 | -1.453432 |
| three | -0.285010 | -1.067717 |           |
| two   | 0.332219  | -1.258206 |           |

可以看到：(['A','B'])成对变成了二级索引

```
df.groupby(['A','B'],as_index=False).mean()
```

|      |    A |     B |         C |         D |
| :--- | ---: | ----: | --------: | --------: |
| 0    |  bar |   one | -0.065097 |  0.066270 |
| 1    |  bar | three |  0.302761 |  0.505803 |
| 2    |  bar |   two | -0.415399 |  0.549753 |
| 3    |  foo |   one |  0.061238 | -1.453432 |
| 4    |  foo | three | -0.285010 | -1.067717 |
| 5    |  foo |   two |  0.332219 | -1.258206 |

##### 1.3. 同时查看多种数据统计

```
df.groupby('A').agg([np.sum,np.mean,np.std])
```

|      |         C |         D |          |           |           |          |
| :--- | --------: | --------: | -------: | --------: | --------: | -------: |
|      |       sum |      mean |      std |       sum |      mean |      std |
| A    |           |           |          |           |           |          |
| bar  | -0.177735 | -0.059245 | 0.359115 |  1.121826 |  0.373942 | 0.267357 |
| foo  |  0.501904 |  0.100381 | 1.170803 | -6.490991 | -1.298198 | 0.366594 |

可以看到：列变成了多级索引

##### 1.4. 查看单列的结果统计数据

```
# 方法1：预过滤，性能更好
df.groupby('A')['C'].agg([np.sum,np.mean,np.std])
```

|      |       sum |      mean |      std |
| :--- | --------: | --------: | -------: |
| A    |           |           |          |
| bar  | -0.177735 | -0.059245 | 0.359115 |
| foo  |  0.501904 |  0.100381 | 1.170803 |

```
# 方法二
df.groupby('A').agg([np.sum,np.mean,np.std])['C']
```

|      |       sum |      mean |      std |
| :--- | --------: | --------: | -------: |
| A    |           |           |          |
| bar  | -0.177735 | -0.059245 | 0.359115 |
| foo  |  0.501904 |  0.100381 | 1.170803 |

##### 1.5. 不同列使用不同的聚合函数

```
df.groupby('A').agg({"C":np.sum,"D":np.mean})
```

|      |         C |         D |
| :--- | --------: | --------: |
| A    |           |           |
| bar  | -0.177735 |  0.373942 |
| foo  |  0.501904 | -1.298198 |

#### 二. 遍历groupby的结果理解执行流程

for循环可以直接遍历每个group

##### 2.1. 遍历单个列聚合的分组

```
g = df.groupby('A')
g
<pandas.core.groupby.groupby.DataFrameGroupBy object at 0x000001AC08EE34A8>
for name,group in g:
    print(name)
    print(group)
    print()
bar
     A      B         C         D
1  bar    one -0.065097  0.066270
3  bar  three  0.302761  0.505803
5  bar    two -0.415399  0.549753

foo
     A      B         C         D
0  foo    one  0.953909 -1.749344
2  foo    two  1.679496 -1.617316
4  foo    two -1.015058 -0.899095
6  foo    one -0.831433 -1.157519
7  foo  three -0.285010 -1.067717
```

***可以获取每个分组的数据***

```
g.get_group('bar')
```

|      |    A |     B |         C |        D |
| :--- | ---: | ----: | --------: | -------: |
| 1    |  bar |   one | -0.065097 | 0.066270 |
| 3    |  bar | three |  0.302761 | 0.505803 |
| 5    |  bar |   two | -0.415399 | 0.549753 |

##### 2.2. 遍历多个列聚合的分组

```
g = df.groupby(['A','B'])
for name,group in g:
    print(name)
    print(group)
    print()
('bar', 'one')
     A    B         C        D
1  bar  one -0.065097  0.06627

('bar', 'three')
     A      B         C         D
3  bar  three  0.302761  0.505803

('bar', 'two')
     A    B         C         D
5  bar  two -0.415399  0.549753

('foo', 'one')
     A    B         C         D
0  foo  one  0.953909 -1.749344
6  foo  one -0.831433 -1.157519

('foo', 'three')
     A      B        C         D
7  foo  three -0.28501 -1.067717

('foo', 'two')
     A    B         C         D
2  foo  two  1.679496 -1.617316
4  foo  two -1.015058 -0.899095
```

可以看到，name是一个2个元素的tuple，代表不同的列

```
g.get_group(('foo','one'))
```

|      |    A |    B |         C |         D |
| :--- | ---: | ---: | --------: | --------: |
| 0    |  foo |  one |  0.953909 | -1.749344 |
| 6    |  foo |  one | -0.831433 | -1.157519 |

***可以直接查询group后的某几列，生成Series或者子DataFrame***

```
g['C']
<pandas.core.groupby.groupby.SeriesGroupBy object at 0x000001AC08ECB5C0>
for name,group in g['C']:
    print(name)
    print(group)
    print(type(group))
    print()
('bar', 'one')
1   -0.065097
Name: C, dtype: float64
<class 'pandas.core.series.Series'>

('bar', 'three')
3    0.302761
Name: C, dtype: float64
<class 'pandas.core.series.Series'>

('bar', 'two')
5   -0.415399
Name: C, dtype: float64
<class 'pandas.core.series.Series'>

('foo', 'one')
0    0.953909
6   -0.831433
Name: C, dtype: float64
<class 'pandas.core.series.Series'>

('foo', 'three')
7   -0.28501
Name: C, dtype: float64
<class 'pandas.core.series.Series'>

('foo', 'two')
2    1.679496
4   -1.015058
Name: C, dtype: float64
<class 'pandas.core.series.Series'>
```

其实所有的聚合统计，都是在DataFrame和Series上进行的;

#### 三. 实例分组探索天气数据

```
df = pd.read_csv("../datas/weather_20230115134249.csv", encoding='utf-8')
#替换温度的后缀℃
df.loc[:,"气温(度)"]=df["气温(度)"].str.replace("℃","").astype("float")
df.head()
```

|      |       日期 |   城市 | 行政区 | 观测站 | 气温(度) | 相对湿度(%) | 累积雨量(mm) |
| :--- | ---------: | -----: | -----: | -----: | -------: | ----------: | -----------: |
| 0    | 2015-01-01 | 新北市 | 烏來區 |   福山 |     13.7 |          92 |          0.0 |
| 1    | 2015-01-02 | 臺南市 | 安平區 |   安平 |     23.5 |          70 |          0.0 |
| 2    | 2015-01-03 | 臺東縣 | 東河鄉 | 七塊厝 |     19.6 |          86 |          0.0 |
| 3    | 2015-01-04 | 新北市 | 貢寮區 |   福隆 |     14.2 |          96 |        -99.0 |
| 4    | 2015-01-05 | 南投縣 | 仁愛鄉 | 小奇萊 |      8.3 |          57 |          0.0 |

```
# 新增一列为月份
df['month'] = df['日期'].str[:7]
df.head()
```

|      |       日期 |   城市 | 行政区 | 观测站 | 气温(度) | 相对湿度(%) | 累积雨量(mm) |   month |
| :--- | ---------: | -----: | -----: | -----: | -------: | ----------: | -----------: | ------: |
| 0    | 2015-01-01 | 新北市 | 烏來區 |   福山 |     13.7 |          92 |          0.0 | 2015-01 |
| 1    | 2015-01-02 | 臺南市 | 安平區 |   安平 |     23.5 |          70 |          0.0 | 2015-01 |
| 2    | 2015-01-03 | 臺東縣 | 東河鄉 | 七塊厝 |     19.6 |          86 |          0.0 | 2015-01 |
| 3    | 2015-01-04 | 新北市 | 貢寮區 |   福隆 |     14.2 |          96 |        -99.0 | 2015-01 |
| 4    | 2015-01-05 | 南投縣 | 仁愛鄉 | 小奇萊 |      8.3 |          57 |          0.0 | 2015-01 |

##### 3.1. 查看每个月的最高温度

```python
data = df.groupby('month')['气温(度)'].max()
data
month
2015-01    27.0
2015-02    28.5
2015-03    29.5
2015-04    29.6
2015-05    27.5
2015-06    26.9
2015-07    30.6
2015-08    27.2
2015-09    28.1
2015-10    29.3
2015-11    26.7
2015-12    28.5
2016-01    28.1
2016-02    27.1
2016-03    29.2
2016-04    28.3
Name: 气温(度), dtype: float64
type(data)
pandas.core.series.Series
data.plot()
<matplotlib.axes._subplots.AxesSubplot at 0x1ac0925dfd0>
```

##### 3.2. 查看每个月的最高温度、平均湿度

```
df.head()
```

|      |       日期 |   城市 | 行政区 | 观测站 | 气温(度) | 相对湿度(%) | 累积雨量(mm) |   month |
| :--- | ---------: | -----: | -----: | -----: | -------: | ----------: | -----------: | ------: |
| 0    | 2015-01-01 | 新北市 | 烏來區 |   福山 |     13.7 |          92 |          0.0 | 2015-01 |
| 1    | 2015-01-02 | 臺南市 | 安平區 |   安平 |     23.5 |          70 |          0.0 | 2015-01 |
| 2    | 2015-01-03 | 臺東縣 | 東河鄉 | 七塊厝 |     19.6 |          86 |          0.0 | 2015-01 |
| 3    | 2015-01-04 | 新北市 | 貢寮區 |   福隆 |     14.2 |          96 |        -99.0 | 2015-01 |
| 4    | 2015-01-05 | 南投縣 | 仁愛鄉 | 小奇萊 |      8.3 |          57 |          0.0 | 2015-01 |

```
group_data = df.groupby('month').agg({'气温(度)':np.max,"相对湿度(%)":np.mean})
group_data
```

|         | 气温(度) | 相对湿度(%) |
| :------ | -------: | ----------: |
| month   |          |             |
| 2015-01 |     27.0 | -568.645161 |
| 2015-02 |     28.5 | -281.178571 |
| 2015-03 |     29.5 |   74.774194 |
| 2015-04 |     29.6 | -254.100000 |
| 2015-05 |     27.5 | -240.838710 |
| 2015-06 |     26.9 | -919.500000 |
| 2015-07 |     30.6 | -247.096774 |
| 2015-08 |     27.2 |   76.935484 |
| 2015-09 |     28.1 |   76.666667 |
| 2015-10 |     29.3 | -243.967742 |
| 2015-11 |     26.7 |   82.466667 |
| 2015-12 |     28.5 |   76.516129 |
| 2016-01 |     28.1 | -238.935484 |
| 2016-02 |     27.1 |   75.482759 |
| 2016-03 |     29.2 | -246.548387 |
| 2016-04 |     28.3 | -279.892857 |

```
group_data.plot()
<matplotlib.axes._subplots.AxesSubplot at 0x1ac08ecb0b8>
```

### Pandas的分层索引

为什么要学习分层索引MultiIndex？

- 分层索引：在一个轴向上拥有多个索引层级，可以表达更高维度数据的形式;
- 可以更方便的进行数据筛选，如果有序则性能更好;
- groupby等操作的结果，如果是多KEY，结果是分层索引，需要会使用;
- 一般不需要自己创建分层索引(MultiIndex有构造函数但一般不用)

演示数据：百度、阿里巴巴、爱奇艺、京东四家公司的股票数据
数据来源：英为财经
[https://cn.investing.com/](https://gitee.com/link?target=https%3A%2F%2Fcn.investing.com%2F)

------

演示提纲:
一. Series的分层索引MultiIndex
二. Series的分层索引怎么筛选数据？
三. DataFrame的多层索引MultiIndex
四. DataFrame有多层索引怎样筛选数据？

------

```python
import pandas as pd
%matplotlib inline
stocks = pd.read_excel('../datas/股票.xlsx')
stocks.shape
(12, 8)
stocks.head()
```

|      | 日期       | 公司 | 收盘  | 开盘  | 高    | 低    | 交易量 | 涨跌幅 |
| :--- | :--------- | :--- | :---- | :---- | :---- | :---- | :----- | :----- |
| 0    | 2023-07-12 | JD   | 37.41 | 37.34 | 37.83 | 36.91 | 11.42M | 0.0386 |
| 1    | 2023-07-11 | JD   | 36.02 | 35.90 | 36.39 | 35.31 | 6.95M  | 0.0019 |
| 2    | 2023-07-10 | JD   | 35.95 | 35.30 | 36.15 | 35.06 | 8.33M  | 0.0053 |
| 3    | 2023-07-12 | BABA | 94.00 | 94.11 | 95.03 | 92.55 | 23.78M | 0.0241 |
| 4    | 2023-07-11 | BABA | 91.79 | 91.02 | 92.32 | 89.01 | 19.74M | 0.0136 |

```python
stocks["公司"].unique()
array(['JD', 'BABA', 'BIDU', 'IQ'], dtype=object)
stocks.index
RangeIndex(start=0, stop=12, step=1)
stocks.groupby('公司')["收盘"].mean()
公司
BABA     92.116667
BIDU    145.036667
IQ        5.350000
JD       36.460000
Name: 收盘, dtype: float64
```

#### 一. Series的分层索引MultiIndex

```
ser = stocks.groupby(['公司','日期'])['收盘'].mean()
ser
公司    日期        
BABA  2023-07-10     90.56
      2023-07-11     91.79
      2023-07-12     94.00
BIDU  2023-07-10    142.95
      2023-07-11    143.33
      2023-07-12    148.83
IQ    2023-07-10      5.12
      2023-07-11      5.23
      2023-07-12      5.70
JD    2023-07-10     35.95
      2023-07-11     36.02
      2023-07-12     37.41
Name: 收盘, dtype: float64
```

------

多维索引中，空白的意思是：使用上面的值

------

```python
ser.index
MultiIndex(levels=[['BABA', 'BIDU', 'IQ', 'JD'], [2023-07-10 00:00:00, 2023-07-11 00:00:00, 2023-07-12 00:00:00]],
           labels=[[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]],
           names=['公司', '日期'])
# unstack把二级索引变成列
ser.unstack()
```

| 日期 | 2023-07-10 00:00:00 | 2023-07-11 00:00:00 | 2023-07-12 00:00:00 |
| :--- | :------------------ | :------------------ | :------------------ |
| 公司 |                     |                     |                     |
| BABA | 90.56               | 91.79               | 94.00               |
| BIDU | 142.95              | 143.33              | 148.83              |
| IQ   | 5.12                | 5.23                | 5.70                |
| JD   | 35.95               | 36.02               | 37.41               |

```python
ser
公司    日期        
BABA  2023-07-10     90.56
      2023-07-11     91.79
      2023-07-12     94.00
BIDU  2023-07-10    142.95
      2023-07-11    143.33
      2023-07-12    148.83
IQ    2023-07-10      5.12
      2023-07-11      5.23
      2023-07-12      5.70
JD    2023-07-10     35.95
      2023-07-11     36.02
      2023-07-12     37.41
Name: 收盘, dtype: float64
ser.reset_index()
```

|      | 公司 | 日期       | 收盘   |
| :--- | :--- | :--------- | :----- |
| 0    | BABA | 2023-07-10 | 90.56  |
| 1    | BABA | 2023-07-11 | 91.79  |
| 2    | BABA | 2023-07-12 | 94.00  |
| 3    | BIDU | 2023-07-10 | 142.95 |
| 4    | BIDU | 2023-07-11 | 143.33 |
| 5    | BIDU | 2023-07-12 | 148.83 |
| 6    | IQ   | 2023-07-10 | 5.12   |
| 7    | IQ   | 2023-07-11 | 5.23   |
| 8    | IQ   | 2023-07-12 | 5.70   |
| 9    | JD   | 2023-07-10 | 35.95  |
| 10   | JD   | 2023-07-11 | 36.02  |
| 11   | JD   | 2023-07-12 | 37.41  |

#### 二. Series的分层索引怎么筛选数据？

```python
ser
公司    日期        
BABA  2023-07-10     90.56
      2023-07-11     91.79
      2023-07-12     94.00
BIDU  2023-07-10    142.95
      2023-07-11    143.33
      2023-07-12    148.83
IQ    2023-07-10      5.12
      2023-07-11      5.23
      2023-07-12      5.70
JD    2023-07-10     35.95
      2023-07-11     36.02
      2023-07-12     37.41
Name: 收盘, dtype: float64
ser.loc['BIDU']
日期
2023-07-10    142.95
2023-07-11    143.33
2023-07-12    148.83
Name: 收盘, dtype: float64
# 多层索引，可以用元组的形式筛选
ser.loc[('BIDU','2023-07-11')]
143.33
ser.loc[:,'2023-07-11']
公司
BABA     91.79
BIDU    143.33
IQ        5.23
JD       36.02
Name: 收盘, dtype: float64
```

#### 三. DataFrame的多层索引MultiIndex

```
stocks.head()
```

|      | 日期       | 公司 | 收盘  | 开盘  | 高    | 低    | 交易量 | 涨跌幅 |
| :--- | :--------- | :--- | :---- | :---- | :---- | :---- | :----- | :----- |
| 0    | 2023-07-12 | JD   | 37.41 | 37.34 | 37.83 | 36.91 | 11.42M | 0.0386 |
| 1    | 2023-07-11 | JD   | 36.02 | 35.90 | 36.39 | 35.31 | 6.95M  | 0.0019 |
| 2    | 2023-07-10 | JD   | 35.95 | 35.30 | 36.15 | 35.06 | 8.33M  | 0.0053 |
| 3    | 2023-07-12 | BABA | 94.00 | 94.11 | 95.03 | 92.55 | 23.78M | 0.0241 |
| 4    | 2023-07-11 | BABA | 91.79 | 91.02 | 92.32 | 89.01 | 19.74M | 0.0136 |

```python
stocks.set_index(['公司','日期'],inplace=True)
stocks
```

|            |            | 收盘   | 开盘   | 高     | 低      | 交易量  | 涨跌幅 |
| :--------- | :--------- | :----- | :----- | :----- | :------ | :------ | :----- |
| 公司       | 日期       |        |        |        |         |         |        |
| JD         | 2023-07-12 | 37.41  | 37.34  | 37.83  | 36.91   | 11.42M  | 0.0386 |
| 2023-07-11 | 36.02      | 35.90  | 36.39  | 35.31  | 6.95M   | 0.0019  |        |
| 2023-07-10 | 35.95      | 35.30  | 36.15  | 35.06  | 8.33M   | 0.0053  |        |
| BABA       | 2023-07-12 | 94.00  | 94.11  | 95.03  | 92.55   | 23.78M  | 0.0241 |
| 2023-07-11 | 91.79      | 91.02  | 92.32  | 89.01  | 19.74M  | 0.0136  |        |
| 2023-07-10 | 90.56      | 90.05  | 92.04  | 89.60  | 25.19M  | 0.0001  |        |
| BIDU       | 2023-07-12 | 148.83 | 147.44 | 150.42 | 145.50  | 2.41M   | 0.0384 |
| 2023-07-11 | 143.33     | 143.23 | 144.45 | 140.01 | 960.45K | 0.0027  |        |
| 2023-07-10 | 142.95     | 140.72 | 143.95 | 140.12 | 887.82K | 0.0020  |        |
| IQ         | 2023-07-12 | 5.70   | 5.43   | 5.78   | 5.41    | 19.24M  | 0.0899 |
| 2023-07-11 | 5.23       | 5.15   | 5.30   | 5.12   | 5.23M   | 0.0215  |        |
| 2023-07-10 | 5.12       | 5.10   | 5.19   | 5.02   | 8.52M   | -0.0078 |        |

```python
stocks.index
MultiIndex(levels=[['BABA', 'BIDU', 'IQ', 'JD'], [2023-07-10 00:00:00, 2023-07-11 00:00:00, 2023-07-12 00:00:00]],
           labels=[[3, 3, 3, 0, 0, 0, 1, 1, 1, 2, 2, 2], [2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0]],
           names=['公司', '日期'])
stocks.sort_index(inplace=True)
stocks
```

|            |            | 收盘   | 开盘   | 高     | 低      | 交易量  | 涨跌幅  |
| :--------- | :--------- | :----- | :----- | :----- | :------ | :------ | :------ |
| 公司       | 日期       |        |        |        |         |         |         |
| BABA       | 2023-07-10 | 90.56  | 90.05  | 92.04  | 89.60   | 25.19M  | 0.0001  |
| 2023-07-11 | 91.79      | 91.02  | 92.32  | 89.01  | 19.74M  | 0.0136  |         |
| 2023-07-12 | 94.00      | 94.11  | 95.03  | 92.55  | 23.78M  | 0.0241  |         |
| BIDU       | 2023-07-10 | 142.95 | 140.72 | 143.95 | 140.12  | 887.82K | 0.0020  |
| 2023-07-11 | 143.33     | 143.23 | 144.45 | 140.01 | 960.45K | 0.0027  |         |
| 2023-07-12 | 148.83     | 147.44 | 150.42 | 145.50 | 2.41M   | 0.0384  |         |
| IQ         | 2023-07-10 | 5.12   | 5.10   | 5.19   | 5.02    | 8.52M   | -0.0078 |
| 2023-07-11 | 5.23       | 5.15   | 5.30   | 5.12   | 5.23M   | 0.0215  |         |
| 2023-07-12 | 5.70       | 5.43   | 5.78   | 5.41   | 19.24M  | 0.0899  |         |
| JD         | 2023-07-10 | 35.95  | 35.30  | 36.15  | 35.06   | 8.33M   | 0.0053  |
| 2023-07-11 | 36.02      | 35.90  | 36.39  | 35.31  | 6.95M   | 0.0019  |         |
| 2023-07-12 | 37.41      | 37.34  | 37.83  | 36.91  | 11.42M  | 0.0386  |         |

#### 四. DataFrame有多层索引怎样筛选数据？

**【重要知识】**在选择数据时：

- 元组(key1,key2)代表筛选多层索引，其中key1是索引第一级，key2是第二级，比如key1=JD，key2=2023-07-11;
- 列表key1，key2代表同一层的多个KEY，其中key1和key2是并列的同级索引，比如key1=JD，key2=BIDU;

------

```python
stocks.loc['BIDU']
```

|            | 收盘   | 开盘   | 高     | 低     | 交易量  | 涨跌幅 |
| :--------- | :----- | :----- | :----- | :----- | :------ | :----- |
| 日期       |        |        |        |        |         |        |
| 2023-07-10 | 142.95 | 140.72 | 143.95 | 140.12 | 887.82K | 0.0020 |
| 2023-07-11 | 143.33 | 143.23 | 144.45 | 140.01 | 960.45K | 0.0027 |
| 2023-07-12 | 148.83 | 147.44 | 150.42 | 145.50 | 2.41M   | 0.0384 |

```python
stocks.loc[('BIDU','2023-07-10'),:]
```

|      |            | 收盘   | 开盘   | 高     | 低     | 交易量  | 涨跌幅 |
| :--- | :--------- | :----- | :----- | :----- | :----- | :------ | :----- |
| 公司 | 日期       |        |        |        |        |         |        |
| BIDU | 2023-07-10 | 142.95 | 140.72 | 143.95 | 140.12 | 887.82K | 0.002  |

```python
stocks.loc[('BIDU','2023-07-10'),'开盘']
公司    日期        
BIDU  2023-07-10    140.72
Name: 开盘, dtype: float64
stocks.loc[['BIDU','JD'],:]
```

|            |            | 收盘   | 开盘   | 高     | 低      | 交易量  | 涨跌幅 |
| :--------- | :--------- | :----- | :----- | :----- | :------ | :------ | :----- |
| 公司       | 日期       |        |        |        |         |         |        |
| BIDU       | 2023-07-10 | 142.95 | 140.72 | 143.95 | 140.12  | 887.82K | 0.0020 |
| 2023-07-11 | 143.33     | 143.23 | 144.45 | 140.01 | 960.45K | 0.0027  |        |
| 2023-07-12 | 148.83     | 147.44 | 150.42 | 145.50 | 2.41M   | 0.0384  |        |
| JD         | 2023-07-10 | 35.95  | 35.30  | 36.15  | 35.06   | 8.33M   | 0.0053 |
| 2023-07-11 | 36.02      | 35.90  | 36.39  | 35.31  | 6.95M   | 0.0019  |        |
| 2023-07-12 | 37.41      | 37.34  | 37.83  | 36.91  | 11.42M  | 0.0386  |        |

```
stocks.loc[(['BIDU','JD'],'2023-07-10'),:]
```

|      |            | 收盘   | 开盘   | 高     | 低     | 交易量  | 涨跌幅 |
| :--- | :--------- | :----- | :----- | :----- | :----- | :------ | :----- |
| 公司 | 日期       |        |        |        |        |         |        |
| BIDU | 2023-07-10 | 142.95 | 140.72 | 143.95 | 140.12 | 887.82K | 0.0020 |
| JD   | 2023-07-10 | 35.95  | 35.30  | 36.15  | 35.06  | 8.33M   | 0.0053 |

```python
stocks.loc[(['BIDU','JD'],'2023-07-10'),'收盘']
公司    日期        
BIDU  2023-07-10    142.95
JD    2023-07-10     35.95
Name: 收盘, dtype: float64
stocks.loc[('BIDU',['2023-07-10','2023-07-11']),'收盘']
公司    日期        
BIDU  2023-07-10    142.95
      2023-07-11    143.33
Name: 收盘, dtype: float64
# slice(None)代表筛选这一索引的所有内容
stocks.loc[(slice(None),['2023-07-10','2023-07-11']),:]
```

|            |            | 收盘   | 开盘   | 高     | 低      | 交易量  | 涨跌幅  |
| :--------- | :--------- | :----- | :----- | :----- | :------ | :------ | :------ |
| 公司       | 日期       |        |        |        |         |         |         |
| BABA       | 2023-07-10 | 90.56  | 90.05  | 92.04  | 89.60   | 25.19M  | 0.0001  |
| 2023-07-11 | 91.79      | 91.02  | 92.32  | 89.01  | 19.74M  | 0.0136  |         |
| BIDU       | 2023-07-10 | 142.95 | 140.72 | 143.95 | 140.12  | 887.82K | 0.0020  |
| 2023-07-11 | 143.33     | 143.23 | 144.45 | 140.01 | 960.45K | 0.0027  |         |
| IQ         | 2023-07-10 | 5.12   | 5.10   | 5.19   | 5.02    | 8.52M   | -0.0078 |
| 2023-07-11 | 5.23       | 5.15   | 5.30   | 5.12   | 5.23M   | 0.0215  |         |
| JD         | 2023-07-10 | 35.95  | 35.30  | 36.15  | 35.06   | 8.33M   | 0.0053  |
| 2023-07-11 | 36.02      | 35.90  | 36.39  | 35.31  | 6.95M   | 0.0019  |         |

```python
stocks.reset_index()
```

|      | 公司 | 日期       | 收盘   | 开盘   | 高     | 低     | 交易量  | 涨跌幅  |
| :--- | :--- | :--------- | :----- | :----- | :----- | :----- | :------ | :------ |
| 0    | BABA | 2023-07-10 | 90.56  | 90.05  | 92.04  | 89.60  | 25.19M  | 0.0001  |
| 1    | BABA | 2023-07-11 | 91.79  | 91.02  | 92.32  | 89.01  | 19.74M  | 0.0136  |
| 2    | BABA | 2023-07-12 | 94.00  | 94.11  | 95.03  | 92.55  | 23.78M  | 0.0241  |
| 3    | BIDU | 2023-07-10 | 142.95 | 140.72 | 143.95 | 140.12 | 887.82K | 0.0020  |
| 4    | BIDU | 2023-07-11 | 143.33 | 143.23 | 144.45 | 140.01 | 960.45K | 0.0027  |
| 5    | BIDU | 2023-07-12 | 148.83 | 147.44 | 150.42 | 145.50 | 2.41M   | 0.0384  |
| 6    | IQ   | 2023-07-10 | 5.12   | 5.10   | 5.19   | 5.02   | 8.52M   | -0.0078 |
| 7    | IQ   | 2023-07-11 | 5.23   | 5.15   | 5.30   | 5.12   | 5.23M   | 0.0215  |
| 8    | IQ   | 2023-07-12 | 5.70   | 5.43   | 5.78   | 5.41   | 19.24M  | 0.0899  |
| 9    | JD   | 2023-07-10 | 35.95  | 35.30  | 36.15  | 35.06  | 8.33M   | 0.0053  |
| 10   | JD   | 2023-07-11 | 36.02  | 35.90  | 36.39  | 35.31  | 6.95M   | 0.0019  |
| 11   | JD   | 2023-07-12 | 37.41  | 37.34  | 37.83  | 36.91  | 11.42M  | 0.0386  |

### Pandas的数据转换函数

数据转换函数对比: map、apply、applymap;

1. map: 只用于Series，实现每个值->值的映射;
2. apply: 用于Series实现每个值的处理，用于DataFrame实现某个轴的Series的处理;
3. applymap: 只能用于DataFrame，用于处理该DataFrame的每个元素;

#### 1. map用于Series值的转换

**实例：将股票代码英文转换为中文名字**

Series.map(dict) or Series.map(function)均可

```python
import pandas as pd
stocks = pd.read_excel('../datas/股票.xlsx')
stocks.head()
```

|      | 日期       | 公司 | 收盘  | 开盘  | 高    | 低    | 交易量 | 涨跌幅 |
| :--- | :--------- | :--- | :---- | :---- | :---- | :---- | :----- | :----- |
| 0    | 2023-07-12 | JD   | 37.41 | 37.34 | 37.83 | 36.91 | 11.42M | 0.0386 |
| 1    | 2023-07-11 | JD   | 36.02 | 35.90 | 36.39 | 35.31 | 6.95M  | 0.0019 |
| 2    | 2023-07-10 | JD   | 35.95 | 35.30 | 36.15 | 35.06 | 8.33M  | 0.0053 |
| 3    | 2023-07-12 | BABA | 94.00 | 94.11 | 95.03 | 92.55 | 23.78M | 0.0241 |
| 4    | 2023-07-11 | BABA | 91.79 | 91.02 | 92.32 | 89.01 | 19.74M | 0.0136 |

```python
stocks['公司'].unique()
array(['JD', 'BABA', 'BIDU', 'IQ'], dtype=object)
# 公司股票代码到中文的映射，注意这里是小写
dict_company_names = {
    "bidu":"百度",
    "baba":"阿里巴巴",
    "iq":"爱奇艺",
    "jd":"京东",
}
**方法1：Series.map(dict) 传入参数为一个字典**
stocks["公司中文1"] = stocks["公司"].str.lower().map(dict_company_names)
stocks.head()
```

|      | 日期       | 公司 | 收盘  | 开盘  | 高    | 低    | 交易量 | 涨跌幅 | 公司中文1 |
| :--- | :--------- | :--- | :---- | :---- | :---- | :---- | :----- | :----- | :-------- |
| 0    | 2023-07-12 | JD   | 37.41 | 37.34 | 37.83 | 36.91 | 11.42M | 0.0386 | 京东      |
| 1    | 2023-07-11 | JD   | 36.02 | 35.90 | 36.39 | 35.31 | 6.95M  | 0.0019 | 京东      |
| 2    | 2023-07-10 | JD   | 35.95 | 35.30 | 36.15 | 35.06 | 8.33M  | 0.0053 | 京东      |
| 3    | 2023-07-12 | BABA | 94.00 | 94.11 | 95.03 | 92.55 | 23.78M | 0.0241 | 阿里巴巴  |
| 4    | 2023-07-11 | BABA | 91.79 | 91.02 | 92.32 | 89.01 | 19.74M | 0.0136 | 阿里巴巴  |

```python
**方法二：Series.map(function) 传入参数为函数**

function的参数是Series的每个元素的值
stocks["公司中文2"] = stocks["公司"].map(lambda x : dict_company_names[x.lower()])
stocks.head()
```

|      | 日期       | 公司 | 收盘  | 开盘  | 高    | 低    | 交易量 | 涨跌幅 | 公司中文1 | 公司中文2 |
| :--- | :--------- | :--- | :---- | :---- | :---- | :---- | :----- | :----- | :-------- | :-------- |
| 0    | 2023-07-12 | JD   | 37.41 | 37.34 | 37.83 | 36.91 | 11.42M | 0.0386 | 京东      | 京东      |
| 1    | 2023-07-11 | JD   | 36.02 | 35.90 | 36.39 | 35.31 | 6.95M  | 0.0019 | 京东      | 京东      |
| 2    | 2023-07-10 | JD   | 35.95 | 35.30 | 36.15 | 35.06 | 8.33M  | 0.0053 | 京东      | 京东      |
| 3    | 2023-07-12 | BABA | 94.00 | 94.11 | 95.03 | 92.55 | 23.78M | 0.0241 | 阿里巴巴  | 阿里巴巴  |
| 4    | 2023-07-11 | BABA | 91.79 | 91.02 | 92.32 | 89.01 | 19.74M | 0.0136 | 阿里巴巴  | 阿里巴巴  |

#### 2. apply用于Series和DataFrame的转换

- Series.apply(**function**),函数的参数是每个值
- DataFrame.apply(**function**),函数的参数是Series

**Series.apply(function)**

function的参数是Series的每个值

------

```python
stocks["公司中文3"] = stocks["公司"].apply(
    lambda x : dict_company_names[x.lower()])
stocks.head()
```

|      | 日期       | 公司 | 收盘  | 开盘  | 高    | 低    | 交易量 | 涨跌幅 | 公司中文1 | 公司中文2 | 公司中文3 |
| :--- | :--------- | :--- | :---- | :---- | :---- | :---- | :----- | :----- | :-------- | :-------- | :-------- |
| 0    | 2023-07-12 | JD   | 37.41 | 37.34 | 37.83 | 36.91 | 11.42M | 0.0386 | 京东      | 京东      | 京东      |
| 1    | 2023-07-11 | JD   | 36.02 | 35.90 | 36.39 | 35.31 | 6.95M  | 0.0019 | 京东      | 京东      | 京东      |
| 2    | 2023-07-10 | JD   | 35.95 | 35.30 | 36.15 | 35.06 | 8.33M  | 0.0053 | 京东      | 京东      | 京东      |
| 3    | 2023-07-12 | BABA | 94.00 | 94.11 | 95.03 | 92.55 | 23.78M | 0.0241 | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  |
| 4    | 2023-07-11 | BABA | 91.79 | 91.02 | 92.32 | 89.01 | 19.74M | 0.0136 | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  |

**DataFrame.apply(function)**

function的参数是对应轴的Series

------

```python
stocks["公司中文4"] = stocks.apply(
    lambda x : dict_company_names[x["公司"].lower()],
    axis=1)
```

注意这个代码：

1. apply实在stocks这个DataFrame上调用;
2. lambda x的x是一个Series，因为指定了axis=1所以Series的key是列名，可以用x['公司']获取;

------

```python
stocks.head()
```

|      | 日期       | 公司 | 收盘  | 开盘  | 高    | 低    | 交易量 | 涨跌幅 | 公司中文1 | 公司中文2 | 公司中文3 | 公司中文4 |
| :--- | :--------- | :--- | :---- | :---- | :---- | :---- | :----- | :----- | :-------- | :-------- | :-------- | :-------- |
| 0    | 2023-07-12 | JD   | 37.41 | 37.34 | 37.83 | 36.91 | 11.42M | 0.0386 | 京东      | 京东      | 京东      | 京东      |
| 1    | 2023-07-11 | JD   | 36.02 | 35.90 | 36.39 | 35.31 | 6.95M  | 0.0019 | 京东      | 京东      | 京东      | 京东      |
| 2    | 2023-07-10 | JD   | 35.95 | 35.30 | 36.15 | 35.06 | 8.33M  | 0.0053 | 京东      | 京东      | 京东      | 京东      |
| 3    | 2023-07-12 | BABA | 94.00 | 94.11 | 95.03 | 92.55 | 23.78M | 0.0241 | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  |
| 4    | 2023-07-11 | BABA | 91.79 | 91.02 | 92.32 | 89.01 | 19.74M | 0.0136 | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  |

#### 3. applymap用于DataFrame所有值的转换

```python
sub_df = stocks[['收盘','开盘','高','低']]
sub_df.head()
```

|      | 收盘  | 开盘  | 高    | 低    |
| :--- | :---- | :---- | :---- | :---- |
| 0    | 37.41 | 37.34 | 37.83 | 36.91 |
| 1    | 36.02 | 35.90 | 36.39 | 35.31 |
| 2    | 35.95 | 35.30 | 36.15 | 35.06 |
| 3    | 94.00 | 94.11 | 95.03 | 92.55 |
| 4    | 91.79 | 91.02 | 92.32 | 89.01 |

```python
# 将这些数字取整数，应用于所有元素
sub_df.applymap(lambda x : int(x))
```

|      | 收盘 | 开盘 | 高   | 低   |
| :--- | :--- | :--- | :--- | :--- |
| 0    | 37   | 37   | 37   | 36   |
| 1    | 36   | 35   | 36   | 35   |
| 2    | 35   | 35   | 36   | 35   |
| 3    | 94   | 94   | 95   | 92   |
| 4    | 91   | 91   | 92   | 89   |
| 5    | 90   | 90   | 92   | 89   |
| 6    | 148  | 147  | 150  | 145  |
| 7    | 143  | 143  | 144  | 140  |
| 8    | 142  | 140  | 143  | 140  |
| 9    | 5    | 5    | 5    | 5    |
| 10   | 5    | 5    | 5    | 5    |
| 11   | 5    | 5    | 5    | 5    |

```python
# 直接修改原df的这几列
stocks.loc[:,['收盘','开盘','高','低']] = sub_df.applymap(lambda x : int(x))
stocks.head()
```

|      | 日期       | 公司 | 收盘 | 开盘 | 高   | 低   | 交易量 | 涨跌幅 | 公司中文1 | 公司中文2 | 公司中文3 | 公司中文4 |
| :--- | :--------- | :--- | :--- | :--- | :--- | :--- | :----- | :----- | :-------- | :-------- | :-------- | :-------- |
| 0    | 2023-07-12 | JD   | 37   | 37   | 37   | 36   | 11.42M | 0.0386 | 京东      | 京东      | 京东      | 京东      |
| 1    | 2023-07-11 | JD   | 36   | 35   | 36   | 35   | 6.95M  | 0.0019 | 京东      | 京东      | 京东      | 京东      |
| 2    | 2023-07-10 | JD   | 35   | 35   | 36   | 35   | 8.33M  | 0.0053 | 京东      | 京东      | 京东      | 京东      |
| 3    | 2023-07-12 | BABA | 94   | 94   | 95   | 92   | 23.78M | 0.0241 | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  |
| 4    | 2023-07-11 | BABA | 91   | 91   | 92   | 89   | 19.74M | 0.0136 | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  | 阿里巴巴  |